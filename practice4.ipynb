{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1601226835588)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, RandomForestClassificationModel}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import scala.util.Random\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier,\n",
    "                                          RandomForestClassifier, RandomForestClassificationModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataWithoutHeader = spark.read.\n",
    "    option(\"inferSchema\", true).\n",
    "    option(\"header\", false).\n",
    "    csv(\"Data/covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.Row = [2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataWithoutHeader.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataWithoutHeader.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydriology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_0: integer (nullable = true)\n",
      " |-- Wilderness_Area_1: integer (nullable = true)\n",
      " |-- Wilderness_Area_2: integer (nullable = true)\n",
      " |-- Wilderness_Area_3: integer (nullable = true)\n",
      " |-- Soil_Type_0: integer (nullable = true)\n",
      " |-- Soil_Type_1: integer (nullable = true)\n",
      " |-- Soil_Type_2: integer (nullable = true)\n",
      " |-- Soil_Type_3: integer (nullable = true)\n",
      " |-- Soil_Type_4: integer (nullable = true)\n",
      " |-- Soil_Type_5: integer (nullable = true)\n",
      " |-- Soil_Type_6: integer (nullable = true)\n",
      " |-- Soil_Type_7: integer (nullable = true)\n",
      " |-- Soil_Type_8: integer (nullable = true)\n",
      " |-- Soil_Type_9: integer (nullable = true)\n",
      " |-- Soil_Type_10: integer (nullable = true)\n",
      " |-- Soil_Type_11: integer (nullable = true)\n",
      " |-- Soil_Type_12: integer (nullable = true)\n",
      " |-- Soil_Type_13: integer (nullable = true)\n",
      " |-- Soil_Type_14: integer (nullable = true)\n",
      " |-- Soil_Type_15: integer (nullable = true)\n",
      " |-- Soil_Type_16: integer (nullable = true)\n",
      " |-- Soil_Type_17: integer (nullable = true)\n",
      " |-- Soil_Type_18: integer (nullable = true)\n",
      " |-- Soil_Type_19: integer (nullable = true)\n",
      " |-- Soil_Type_20: integer (nullable = true)\n",
      " |-- Soil_Type_21: integer (nullable = true)\n",
      " |-- Soil_Type_22: integer (nullable = true)\n",
      " |-- Soil_Type_23: integer (nullable = true)\n",
      " |-- Soil_Type_24: integer (nullable = true)\n",
      " |-- Soil_Type_25: integer (nullable = true)\n",
      " |-- Soil_Type_26: integer (nullable = true)\n",
      " |-- Soil_Type_27: integer (nullable = true)\n",
      " |-- Soil_Type_28: integer (nullable = true)\n",
      " |-- Soil_Type_29: integer (nullable = true)\n",
      " |-- Soil_Type_30: integer (nullable = true)\n",
      " |-- Soil_Type_31: integer (nullable = true)\n",
      " |-- Soil_Type_32: integer (nullable = true)\n",
      " |-- Soil_Type_33: integer (nullable = true)\n",
      " |-- Soil_Type_34: integer (nullable = true)\n",
      " |-- Soil_Type_35: integer (nullable = true)\n",
      " |-- Soil_Type_36: integer (nullable = true)\n",
      " |-- Soil_Type_37: integer (nullable = true)\n",
      " |-- Soil_Type_38: integer (nullable = true)\n",
      " |-- Soil_Type_39: integer (nullable = true)\n",
      " |-- Cover_Type: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Seq[String] = List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydriology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames = Seq(\n",
    "    \"Elevation\",\"Aspect\",\"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydriology\",\"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\") ++\n",
    "    ((0 until 4).map(i => s\"Wilderness_Area_$i\")) ++\n",
    "    ((0 until 40).map(i=> s\"Soil_Type_$i\")) ++ Seq(\"Cover_Type\")\n",
    "\n",
    "val data = dataWithoutHeader.toDF(colNames:_*).withColumn(\"Cover_Type\",$\"Cover_Type\".cast(\"double\"))\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "res3: testData.type = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData, testData) = data.randomSplit(Array(0.9,0.1))\n",
    "trainData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydriology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputCols = trainData.columns.filter(_ != \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1859.0,18.0,12.0,67.0,11.0,90.0,211.0,215.0,139.0,792.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1860.0,18.0,13.0,95.0,15.0,90.0,210.0,213.0,138.0,780.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1861.0,35.0,14.0,60.0,11.0,85.0,218.0,209.0,124.0,832.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1866.0,23.0,14.0,85.0,16.0,108.0,212.0,210.0,133.0,819.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1867.0,20.0,15.0,108.0,19.0,120.0,208.0,206.0,132.0,808.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1868.0,27.0,16.0,67.0,17.0,95.0,212.0,204.0,125.0,859.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1871.0,22.0,22.0,60.0,12.0,85.0,200.0,187.0,115.0,792.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1871.0,36.0,19.0,134.0,26.0,120.0,215.0,194.0,107.0,797.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1871.0,37.0,19.0,120.0,29.0,90.0,216.0,195.0,107.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1872.0,12.0,27.0,85.0,25.0,60.0,182.0,174.0,118.0,577.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,16.0,95.0,22.0,124.0,212.0,205.0,126.0,847.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,21.0,108.0,30.0,67.0,206.0,190.0,112.0,713.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1872.0,35.0,21.0,120.0,18.0,85.0,213.0,189.0,104.0,797.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1873.0,30.0,19.0,67.0,21.0,85.0,211.0,195.0,114.0,899.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1876.0,25.0,17.0,124.0,26.0,150.0,209.0,200.0,123.0,836.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1876.0,29.0,19.0,124.0,34.0,90.0,210.0,195.0,115.0,750.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1877.0,27.0,24.0,90.0,18.0,95.0,201.0,179.0,104.0,780.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1877.0,28.0,22.0,127.0,35.0,85.0,205.0,185.0,107.0,706.0,1.0,1.0]) |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "assembledTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 54 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembledTrainData = assembler.transform(trainData)\n",
    "assembledTrainData.select(\"featureVector\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_0e16dd4bea91\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=dtc_0e16dd4bea91) of depth 5 with 49 nodes\n",
      "  If (feature 0 <= 3053.5)\n",
      "   If (feature 0 <= 2479.5)\n",
      "    If (feature 3 <= 15.0)\n",
      "     If (feature 12 <= 0.5)\n",
      "      If (feature 23 <= 0.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 23 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 12 > 0.5)\n",
      "      Predict: 6.0\n",
      "    Else (feature 3 > 15.0)\n",
      "     If (feature 16 <= 0.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 16 > 0.5)\n",
      "      If (feature 9 <= 1345.0)\n",
      "       Predict: 3.0\n",
      "      Else (feature 9 > 1345.0)\n",
      "       Predict: 4.0\n",
      "   Else (feature 0 > 2479.5)\n",
      "    If (feature 17 <= 0.5)\n",
      "     If (feature 0 <= 2959.5)\n",
      "      If (feature 15 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 15 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2959.5)\n",
      "      Predict: 2.0\n",
      "    Else (feature 17 > 0.5)\n",
      "     If (feature 0 <= 2691.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 0 > 2691.5)\n",
      "      If (feature 5 <= 1192.5)\n",
      "       Predict: 5.0\n",
      "      Else (feature 5 > 1192.5)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3053.5)\n",
      "   If (feature 0 <= 3299.5)\n",
      "    If (feature 7 <= 240.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 4100.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 4100.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 7 > 240.5)\n",
      "     If (feature 3 <= 360.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 360.5)\n",
      "      Predict: 2.0\n",
      "   Else (feature 0 > 3299.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 10 <= 0.5)\n",
      "      If (feature 0 <= 3435.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 3435.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 10 > 0.5)\n",
      "      If (feature 9 <= 3217.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 9 > 3217.0)\n",
      "       Predict: 7.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      If (feature 0 <= 3351.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 3351.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 998.0)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 998.0)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_0e16dd4bea91) of depth 5 with 49 nodes\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = classifier.fit(assembledTrainData)\n",
    "println(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8176408960344523,Elevation)\n",
      "(0.038934316703059905,Soil_Type_3)\n",
      "(0.03078040510960056,Soil_Type_1)\n",
      "(0.0277844260299692,Soil_Type_31)\n",
      "(0.02558996898858634,Hillshade_Noon)\n",
      "(0.01835509478117813,Horizontal_Distance_To_Hydriology)\n",
      "(0.01472283138596372,Wilderness_Area_2)\n",
      "(0.01072253211836195,Horizontal_Distance_To_Fire_Points)\n",
      "(0.00791760083420736,Horizontal_Distance_To_Roadways)\n",
      "(0.0034203438368014456,Soil_Type_2)\n",
      "(0.003132419635918021,Wilderness_Area_0)\n",
      "(9.991645419009118E-4,Soil_Type_9)\n",
      "(0.0,Wilderness_Area_3)\n",
      "(0.0,Wilderness_Area_1)\n",
      "(0.0,Vertical_Distance_To_Hydrology)\n",
      "(0.0,Soil_Type_8)\n",
      "(0.0,Soil_Type_7)\n",
      "(0.0,Soil_Type_6)\n",
      "(0.0,Soil_Type_5)\n",
      "(0.0,Soil_Type_4)\n",
      "(0.0,Soil_Type_39)\n",
      "(0.0,Soil_Type_38)\n",
      "(0.0,Soil_Type_37)\n",
      "(0.0,Soil_Type_36)\n",
      "(0.0,Soil_Type_35)\n",
      "(0.0,Soil_Type_34)\n",
      "(0.0,Soil_Type_33)\n",
      "(0.0,Soil_Type_32)\n",
      "(0.0,Soil_Type_30)\n",
      "(0.0,Soil_Type_29)\n",
      "(0.0,Soil_Type_28)\n",
      "(0.0,Soil_Type_27)\n",
      "(0.0,Soil_Type_26)\n",
      "(0.0,Soil_Type_25)\n",
      "(0.0,Soil_Type_24)\n",
      "(0.0,Soil_Type_23)\n",
      "(0.0,Soil_Type_22)\n",
      "(0.0,Soil_Type_21)\n",
      "(0.0,Soil_Type_20)\n",
      "(0.0,Soil_Type_19)\n",
      "(0.0,Soil_Type_18)\n",
      "(0.0,Soil_Type_17)\n",
      "(0.0,Soil_Type_16)\n",
      "(0.0,Soil_Type_15)\n",
      "(0.0,Soil_Type_14)\n",
      "(0.0,Soil_Type_13)\n",
      "(0.0,Soil_Type_12)\n",
      "(0.0,Soil_Type_11)\n",
      "(0.0,Soil_Type_10)\n",
      "(0.0,Soil_Type_0)\n",
      "(0.0,Slope)\n",
      "(0.0,Hillshade_9am)\n",
      "(0.0,Hillshade_3pm)\n",
      "(0.0,Aspect)\n"
     ]
    }
   ],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).\n",
    "    sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                      |\n",
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.023454157782515993,0.24946695095948826,0.6368159203980099,0.0,0.09026297085998579,0.0]                |\n",
      "|3.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.391670058336725E-5,0.049789716456383123,0.6200312033645367,0.02204585537918871,0.0,0.3080993080993081,0.0]|\n",
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 57 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(assembledTrainData)\n",
    "\n",
    "predictions.select(\"Cover_Type\",\"prediction\",\"probability\").\n",
    "    show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_5ba7811d2ce8\n",
       "accuracy: Double = 0.7050595164088547\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "val accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121508.0  66835.0   74.0     0.0     40.0   3.0    2253.0  \n",
      "41945.0   209414.0  2812.0   33.0    334.0  41.0   250.0   \n",
      "0.0       4606.0    27039.0  354.0   42.0   96.0   0.0     \n",
      "0.0       8.0       1324.0   1141.0  0.0    0.0    0.0     \n",
      "0.0       7926.0    276.0    0.0     411.0  0.0    0.0     \n",
      "0.0       4973.0    10085.0  127.0   14.0   427.0  0.0     \n",
      "9514.0    229.0     0.0      0.0     0.0    0.0    8663.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictionRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[89] at rdd at <console>:40\n",
       "multiclassMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@5a64687a\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionRDD = predictions.\n",
    "    select(\"prediction\",\"Cover_Type\").\n",
    "    as[(Double,Double)].rdd\n",
    "val multiclassMetrics = new MulticlassMetrics(predictionRDD)\n",
    "println(multiclassMetrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifierHyper: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_74d8eb61da53\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_d3cfc3c2f311\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifierHyper = new DecisionTreeClassifier().\n",
    "setSeed(Random.nextLong()).\n",
    "setLabelCol(\"Cover_Type\").\n",
    "setFeaturesCol(\"featureVector\").\n",
    "setPredictionCol(\"prediction\")\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, classifierHyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tdtc_74d8eb61da53-impurity: gini,\n",
       "\tdtc_74d8eb61da53-maxBins: 40,\n",
       "\tdtc_74d8eb61da53-maxDepth: 1,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
       "}, {\n",
       "\tdtc_74d8eb61da53-impurity: gini,\n",
       "\tdtc_74d8eb61da53-maxBins: 80,\n",
       "\tdtc_74d8eb61da53-maxDepth: 1,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
       "}, {\n",
       "\tdtc_74d8eb61da53-impurity: gini,\n",
       "\tdtc_74d8eb61da53-maxBins: 40,\n",
       "\tdtc_74d8eb61da53-maxDepth: 1,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
       "}, {\n",
       "\tdtc_74d8eb61da53-impurity: gini,\n",
       "\tdtc_74d8eb61da53-maxBins: 80,\n",
       "\tdtc_74d8eb61da53-maxDepth: 1,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
       "}, {\n",
       "\tdtc_74d8eb61da53-impurity: entropy,\n",
       "\tdtc_74d8eb61da53-maxBins: 40,\n",
       "\tdtc_74d8eb61da53-maxDepth: 1,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
       "}, {\n",
       "\tdtc_74d8eb61da53-impurity: entropy,\n",
       "\tdtc_74d8eb6..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(classifierHyper.impurity, Seq(\"gini\",\"entropy\")).\n",
    "addGrid(classifierHyper.maxDepth, Seq(1,5)).\n",
    "addGrid(classifierHyper.maxBins, Seq(40,80)).\n",
    "addGrid(classifierHyper.minInfoGain, Seq(0.0,0.05)).\n",
    "build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiclassEval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_b7d94e3a779e\n",
       "validator: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_a301630f2fdd\n",
       "validatorModel: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_a301630f2fdd\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "setLabelCol(\"Cover_Type\").\n",
    "setPredictionCol(\"prediction\").\n",
    "setMetricName(\"accuracy\")\n",
    "\n",
    "val validator = new TrainValidationSplit().\n",
    "setSeed(Random.nextLong()).\n",
    "setEstimator(pipeline).\n",
    "setEvaluator(multiclassEval).\n",
    "setEstimatorParamMaps(paramGrid).\n",
    "setTrainRatio(0.9)\n",
    "\n",
    "val validatorModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestModel: org.apache.spark.ml.Model[_] = pipeline_d3cfc3c2f311\n",
       "res9: org.apache.spark.ml.param.ParamMap =\n",
       "{\n",
       "\tdtc_74d8eb61da53-cacheNodeIds: false,\n",
       "\tdtc_74d8eb61da53-checkpointInterval: 10,\n",
       "\tdtc_74d8eb61da53-featuresCol: featureVector,\n",
       "\tdtc_74d8eb61da53-impurity: gini,\n",
       "\tdtc_74d8eb61da53-labelCol: Cover_Type,\n",
       "\tdtc_74d8eb61da53-maxBins: 40,\n",
       "\tdtc_74d8eb61da53-maxDepth: 5,\n",
       "\tdtc_74d8eb61da53-maxMemoryInMB: 256,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0,\n",
       "\tdtc_74d8eb61da53-minInstancesPerNode: 1,\n",
       "\tdtc_74d8eb61da53-predictionCol: prediction,\n",
       "\tdtc_74d8eb61da53-probabilityCol: probability,\n",
       "\tdtc_74d8eb61da53-rawPredictionCol: rawPrediction,\n",
       "\tdtc_74d8eb61da53-seed: 2196387991364154618\n",
       "}\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bestModel = validatorModel.bestModel\n",
    "bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7037961079338272\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.7028365255440833\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.702721375657314\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.6987678962115688\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.6918972863010018\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6883084481633593\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6706713238398649\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6689632671861206\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 5,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6346294092810809\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.6346294092810809\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.6329213526273366\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.6329213526273366\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: gini,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.4901930679768165\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.4901930679768165\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
      "}\n",
      "\n",
      "0.4901930679768165\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 40,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n",
      "0.4901930679768165\n",
      "{\n",
      "\tdtc_74d8eb61da53-impurity: entropy,\n",
      "\tdtc_74d8eb61da53-maxBins: 80,\n",
      "\tdtc_74d8eb61da53-maxDepth: 1,\n",
      "\tdtc_74d8eb61da53-minInfoGain: 0.05\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramAndMetrics: Array[(Double, org.apache.spark.ml.param.ParamMap)] =\n",
       "Array((0.7037961079338272,{\n",
       "\tdtc_74d8eb61da53-impurity: gini,\n",
       "\tdtc_74d8eb61da53-maxBins: 40,\n",
       "\tdtc_74d8eb61da53-maxDepth: 5,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
       "}), (0.7028365255440833,{\n",
       "\tdtc_74d8eb61da53-impurity: gini,\n",
       "\tdtc_74d8eb61da53-maxBins: 80,\n",
       "\tdtc_74d8eb61da53-maxDepth: 5,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
       "}), (0.702721375657314,{\n",
       "\tdtc_74d8eb61da53-impurity: entropy,\n",
       "\tdtc_74d8eb61da53-maxBins: 80,\n",
       "\tdtc_74d8eb61da53-maxDepth: 5,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
       "}), (0.6987678962115688,{\n",
       "\tdtc_74d8eb61da53-impurity: entropy,\n",
       "\tdtc_74d8eb61da53-maxBins: 40,\n",
       "\tdtc_74d8eb61da53-maxDepth: 5,\n",
       "\tdtc_74d8eb61da53-minInfoGain: 0.0\n",
       "}), (0.6918972863010018,{\n",
       "\tdtc_74d8eb61da53-impurity: entropy,\n",
       "\tdtc_74d8eb61da53-maxBins: 8..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramAndMetrics = validatorModel.validationMetrics.\n",
    "zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)\n",
    "paramAndMetrics.foreach{ case (metric, params) => \n",
    "println(metric)\n",
    "println(params)\n",
    "println()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Double = 0.7053680322940823\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validatorModel.validationMetrics.max\n",
    "multiclassEval.evaluate(bestModel.transform(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
