{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1602442574241)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sqlC: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@47b4302c\n",
       "import sqlC.implicits._\n",
       "data: org.apache.spark.sql.DataFrame = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._\n",
    "\n",
    "val data = spark.read.\n",
    "        option(\"inferSchema\",true).\n",
    "        option(\"header\",false).\n",
    "        csv(\"Data/kddcup.data_10_percent_corrected\").\n",
    "        toDF(\n",
    "        \"duration\",\"protocol_type\",\"service\",\"flag\",\n",
    "        \"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\n",
    "        \"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n",
    "        \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n",
    "        \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "        \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\n",
    "        \"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\n",
    "        \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\n",
    "        \"dst_host_count\",\"dst_host_srv_count\",\n",
    "        \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\n",
    "        \"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\n",
    "        \"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "        \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\n",
    "        \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: data.type = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"label\").groupBy(\"label\").count().orderBy($\"count\".desc).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericOnly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [duration: int, src_bytes: int ... 37 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericOnly = data.drop(\"protocol_type\",\"service\",\"flag\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
       "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
       "import org.apache.spark.ml.feature.{OneHotEncoder, VectorAssembler, StringIndexer, StandardScaler}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import scala.util.Random\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{PipelineModel, Pipeline}\n",
    "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder, VectorAssembler, StringIndexer, StandardScaler}\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8ab1076fa390\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_e996e7c986bb\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_a9efab245575\n",
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_a9efab245575\n",
       "kmeansModel: org.apache.spark.ml.clustering.KMeansModel = kmeans_e996e7c986bb\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(numericOnly.columns.filter(_ !=\"label\")).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val kmeans = new KMeans().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setPredictionCol(\"cluster\").\n",
    "    setFeaturesCol(\"featureVector\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "val pipelineModel = pipeline.fit(numericOnly)\n",
    "val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.979395571029514,1622.078830816566,868.5341828266062,4.453261001578883E-5,0.006432937937735314,1.4169466823205539E-5,0.03451682118132869,1.5181571596291647E-4,0.14824703453301485,0.01021213716043885,1.1133152503947209E-4,3.6435771831099954E-5,0.011351767134933808,0.0010829521072021374,1.0930731549329986E-4,0.0010080563539937655,0.0,0.0,0.0013865835391279706,332.2862475203433,292.9071434354884,0.17668541759442943,0.17660780940042914,0.05743309987449898,0.05771839196793656,0.7915488441762945,0.020981640419421355,0.028996862475203923,232.4707319541719,188.6660459090725,0.7537812031901686,0.030905611108870867,0.6019355289259973,0.006683514837454898,0.17675395732966057,0.1764416217966883,0.05811762681672766,0.057411116958826745]\n",
      "[2.0,6.9337564E8,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,57.0,3.0,0.79,0.67,0.21,0.33,0.05,0.39,0.0,255.0,3.0,0.01,0.09,0.22,0.0,0.18,0.67,0.05,0.33]\n"
     ]
    }
   ],
   "source": [
    "kmeansModel.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|          smurf.|280790|\n",
      "|      0|        neptune.|107201|\n",
      "|      0|         normal.| 97278|\n",
      "|      0|           back.|  2203|\n",
      "|      0|          satan.|  1589|\n",
      "|      0|        ipsweep.|  1247|\n",
      "|      0|      portsweep.|  1039|\n",
      "|      0|    warezclient.|  1020|\n",
      "|      0|       teardrop.|   979|\n",
      "|      0|            pod.|   264|\n",
      "|      0|           nmap.|   231|\n",
      "|      0|   guess_passwd.|    53|\n",
      "|      0|buffer_overflow.|    30|\n",
      "|      0|           land.|    21|\n",
      "|      0|    warezmaster.|    20|\n",
      "|      0|           imap.|    12|\n",
      "|      0|        rootkit.|    10|\n",
      "|      0|     loadmodule.|     9|\n",
      "|      0|      ftp_write.|     8|\n",
      "|      0|       multihop.|     7|\n",
      "|      0|            phf.|     4|\n",
      "|      0|           perl.|     3|\n",
      "|      0|            spy.|     2|\n",
      "|      1|      portsweep.|     1|\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withCluster: org.apache.spark.sql.DataFrame = [duration: int, src_bytes: int ... 39 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withCluster = pipelineModel.transform(numericOnly)\n",
    "\n",
    "withCluster.select(\"cluster\",\"label\").\n",
    "    groupBy(\"cluster\",\"label\").count().\n",
    "    orderBy($\"cluster\",$\"count\".desc).\n",
    "    show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore0: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clusteringScore0(data: DataFrame, k: Int): Double = {\n",
    "    val assembler = new VectorAssembler().\n",
    "    setInputCols(data.columns.filter(_ !=\"label\")).\n",
    "    setOutputCol(\"featureVector\")\n",
    "    \n",
    "    val kmeans =  new KMeans().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setK(k).\n",
    "        setPredictionCol(\"cluster\").\n",
    "        setFeaturesCol(\"featureVector\")\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "    \n",
    "    val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(assembler.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,6.025019577677537E7)\n",
      "(40,3.4160392678435594E7)\n",
      "(60,1.0904643275915088E7)\n",
      "(80,4706517.614093637)\n",
      "(100,3.1359282839813825E7)\n"
     ]
    }
   ],
   "source": [
    "(20 to 100 by 20).map(k => (k, clusteringScore0(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore1: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clusteringScore1(data: DataFrame, k: Int): Double = {\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(data.columns.filter(_ !=\"label\")).\n",
    "        setOutputCol(\"featureVector\")\n",
    "    \n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setK(k).\n",
    "        setPredictionCol(\"cluster\").\n",
    "        setFeaturesCol(\"featureVector\").\n",
    "        setMaxIter(40).\n",
    "        setTol(1.0e-5)\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, kmeans))\n",
    "    \n",
    "    val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(assembler.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,3.882886309009383E7)\n",
      "(40,7065963.035438348)\n",
      "(60,6368375.651097387)\n",
      "(80,6882277.580070589)\n",
      "(100,3024476.432966163)\n"
     ]
    }
   ],
   "source": [
    "(20 to 100 by 20).map(k => (k, clusteringScore1(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore2: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clusteringScore2(data: DataFrame, k: Int): Double = {\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(data.columns.filter(_ !=\"label\")).\n",
    "        setOutputCol(\"featureVector\")\n",
    "    \n",
    "    val scaler = new StandardScaler()\n",
    "        .setInputCol(\"featureVector\")\n",
    "        .setOutputCol(\"scaledFeatureVector\")\n",
    "        .setWithStd(true)\n",
    "        .setWithMean(false)\n",
    "    \n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setK(k).\n",
    "        setPredictionCol(\"cluster\").\n",
    "        setFeaturesCol(\"scaledFeatureVector\").\n",
    "        setMaxIter(40).\n",
    "        setTol(1.0e-5)\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(Array(assembler, scaler, kmeans))\n",
    "    val pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(pipelineModel.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60,1.0951688035838902)\n",
      "(90,0.6711836985503741)\n",
      "(120,0.46290733670281287)\n",
      "(150,0.34974018415248276)\n",
      "(180,0.30119968215943305)\n",
      "(210,0.2588374801014279)\n",
      "(240,0.22292602979307774)\n",
      "(270,0.20750607898020548)\n"
     ]
    }
   ],
   "source": [
    "(60 to 270 by 30).map(k => (k, clusteringScore2(numericOnly, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oneHotPipeline: (inputCol: String)(org.apache.spark.ml.Pipeline, String)\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oneHotPipeline(inputCol: String): (Pipeline, String) = {\n",
    "    val indexer = new StringIndexer().\n",
    "        setInputCol(inputCol).\n",
    "        setOutputCol(inputCol + \"_indexed\")\n",
    "    val encoder = new OneHotEncoder().\n",
    "        setInputCol(inputCol + \"_indexed\").\n",
    "        setOutputCol(inputCol + \"_vec\")\n",
    "    val pipeline = new Pipeline().setStages(Array(indexer, encoder))\n",
    "    (pipeline, inputCol + \"_vec\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusteringScore3: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clusteringScore3(data: DataFrame, k: Int): Double = {\n",
    "    val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
    "    val (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
    "    val (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")    \n",
    "    \n",
    "    val assembleCols = Set(data.columns: _*) --\n",
    "        Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n",
    "        Seq(protoTypeVecCol, serviceVecCol, flagVecCol)  \n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(assembleCols.toArray).\n",
    "        setOutputCol(\"featureVector\")\n",
    "    \n",
    "    val scaler = new StandardScaler()\n",
    "        .setInputCol(\"featureVector\")\n",
    "        .setOutputCol(\"scaledFeatureVector\")\n",
    "        .setWithStd(true)\n",
    "        .setWithMean(false)\n",
    "    \n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setK(k).\n",
    "        setPredictionCol(\"cluster\").\n",
    "        setFeaturesCol(\"scaledFeatureVector\").\n",
    "        setMaxIter(40).\n",
    "        setTol(1.0e-5)\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(\n",
    "        Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))\n",
    "    val pipelineModel = pipeline.fit(data)\n",
    "    \n",
    "    val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(pipelineModel.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60,34.59853768509183)\n",
      "(90,9.655109805623399)\n",
      "(120,3.025799941534635)\n",
      "(150,1.9387075439611636)\n",
      "(180,1.4530838535393191)\n",
      "(210,1.6127464886594087)\n",
      "(240,0.9991314756117768)\n",
      "(270,0.771802982870259)\n"
     ]
    }
   ],
   "source": [
    "(60 to 270 by 30).map(k => (k, clusteringScore3(data, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitPipeline4: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fitPipeline4(data: DataFrame, k: Int): PipelineModel = {\n",
    "    val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
    "    val (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
    "    val (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")    \n",
    "    \n",
    "    val assembleCols = Set(data.columns: _*) --\n",
    "        Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n",
    "        Seq(protoTypeVecCol, serviceVecCol, flagVecCol)  \n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(assembleCols.toArray).\n",
    "        setOutputCol(\"featureVector\")\n",
    "    \n",
    "    val scaler = new StandardScaler()\n",
    "        .setInputCol(\"featureVector\")\n",
    "        .setOutputCol(\"scaledFeatureVector\")\n",
    "        .setWithStd(true)\n",
    "        .setWithMean(false)\n",
    "    \n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setK(k).\n",
    "        setPredictionCol(\"cluster\").\n",
    "        setFeaturesCol(\"scaledFeatureVector\").\n",
    "        setMaxIter(40).\n",
    "        setTol(1.0e-5)\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(\n",
    "        Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))\n",
    "    pipeline.fit(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipelineModel = fitPipeline4(data,120)\n",
    "val countByClusterLabel = pipelineModel.transform(data).\n",
    "    select(\"cluster\",\"label\").\n",
    "    groupBy(\"cluster\",\"label\").count().\n",
    "    orderBy(\"cluster\",\"label\")\n",
    "countByClusterLabel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val kMeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "val centroids = kMeansModel.clusterCenters\n",
    "\n",
    "val clustered = pipelineModel.transform(data)\n",
    "val threshold = clustered.\n",
    "    select(\"cluster\",\"scaledFeatureVector\").as[(Int, Vector)].\n",
    "    map { case (cluster, vec) => Vectors.sqdist(centroids(cluster), vec)}.\n",
    "    orderBy($\"value\".desc).take(100).last\n",
    "\n",
    "val originalCols = data.columns\n",
    "val anomalies = clustered.filter {row =>\n",
    "    val cluster = row.getAs[Int](\"cluster\")\n",
    "    val vec = row.getAs[Vector](\"scaledFeatureVector\")\n",
    "    Vectors.sqdist(centroids(cluster),vec) >= threshold\n",
    "}.select(originalCols.head, originalCols.tail:_*)\n",
    "\n",
    "println(anomalies.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
