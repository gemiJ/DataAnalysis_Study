{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1603053752604)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import edu.umd.cloud9.collection.XMLInputFormat\n",
       "import org.apache.spark.sql.{Dataset, SparkSession, Row}\n",
       "import org.apache.hadoop.io.{Text, LongWritable}\n",
       "import org.apache.hadoop.conf.Configuration\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import edu.umd.cloud9.collection.XMLInputFormat\n",
    "import org.apache.spark.sql.{ Dataset, SparkSession,Row }\n",
    "import org.apache.hadoop.io.{ Text, LongWritable }\n",
    "import org.apache.hadoop.conf.Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadMedline: (spark: org.apache.spark.sql.SparkSession, path: String)org.apache.spark.sql.Dataset[String]\n",
       "medlineRaw: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadMedline(spark: SparkSession, path: String) = {\n",
    "    import spark.implicits._\n",
    "    @transient val conf = new Configuration()\n",
    "    conf.set(XMLInputFormat.START_TAG_KEY, \"<MedlineCitation \")\n",
    "    conf.set(XMLInputFormat.END_TAG_KEY, \"</MedlineCitation>\")\n",
    "    val sc = spark.sparkContext\n",
    "    val in = sc.newAPIHadoopFile(path, classOf[XMLInputFormat],\n",
    "                                classOf[LongWritable], classOf[Text], conf)\n",
    "    in.map(line => line._2.toString).toDS()\n",
    "}\n",
    "val medlineRaw = loadMedline(spark, \"Data/medline_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.xml._\n",
       "rawXml: String =\n",
       "<MedlineCitation Owner=\"PIP\" Status=\"MEDLINE\">\n",
       "<PMID Version=\"1\">12255379</PMID>\n",
       "<DateCreated>\n",
       "<Year>1980</Year>\n",
       "<Month>01</Month>\n",
       "<Day>03</Day>\n",
       "</DateCreated>\n",
       "<DateCompleted>\n",
       "<Year>1980</Year>\n",
       "<Month>01</Month>\n",
       "<Day>03</Day>\n",
       "</DateCompleted>\n",
       "<DateRevised>\n",
       "<Year>2013</Year>\n",
       "<Month>02</Month>\n",
       "<Day>19</Day>\n",
       "</DateRevised>\n",
       "<Article PubModel=\"Print\">\n",
       "<Journal>\n",
       "<ISSN IssnType=\"Print\">0002-9955</ISSN>\n",
       "<JournalIssue CitedMedium=\"Print\">\n",
       "<Volume>159</Volume>\n",
       "<Issue>3</Issue>\n",
       "<PubDate>\n",
       "<Year>1955</Year>\n",
       "<Month>Sep</Month>\n",
       "<Day>17</Day>\n",
       "</PubDate>\n",
       "</JournalIssue>\n",
       "<Title>Journal of the American Medical Association</Title>\n",
       "<ISOAbbreviation>J Am Med Assoc</ISOAbbreviation>\n",
       "</Journal>\n",
       "<ArticleTitle>Association of maternal and fetal factors with development of menta..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.xml._\n",
    "val rawXml = medlineRaw.take(1)(0)\n",
    "val elem = XML.loadString(rawXml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: String = MedlineCitation\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: scala.xml.MetaData =  Status=\"MEDLINE\" Owner=\"PIP\"\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem.attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: scala.xml.NodeSeq =\n",
       "NodeSeq(<MeshHeadingList>\n",
       "<MeshHeading>\n",
       "<DescriptorName UI=\"D001519\" MajorTopicYN=\"N\">Behavior</DescriptorName>\n",
       "</MeshHeading>\n",
       "<MeshHeading>\n",
       "<DescriptorName UI=\"D000013\" MajorTopicYN=\"N\">Congenital Abnormalities</DescriptorName>\n",
       "</MeshHeading>\n",
       "<MeshHeading>\n",
       "<DescriptorName UI=\"D006233\" MajorTopicYN=\"N\">Disabled Persons</DescriptorName>\n",
       "</MeshHeading>\n",
       "<MeshHeading>\n",
       "<DescriptorName UI=\"D004194\" MajorTopicYN=\"N\">Disease</DescriptorName>\n",
       "</MeshHeading>\n",
       "<MeshHeading>\n",
       "<DescriptorName UI=\"D008607\" MajorTopicYN=\"Y\">Intellectual Disability</DescriptorName>\n",
       "</MeshHeading>\n",
       "<MeshHeading>\n",
       "<DescriptorName UI=\"D007360\" MajorTopicYN=\"N\">Intelligence</DescriptorName>\n",
       "</MeshHeading>\n",
       "<MeshHeading>\n",
       "<DescriptorName UI=\"D008431\" MajorTopicYN=\"Y\">Maternal-Fetal Exchange</DescriptorNa..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem \\ \"MeshHeadingList\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: scala.xml.NodeSeq =\n",
       "NodeSeq(<MeshHeading>\n",
       "<DescriptorName UI=\"D001519\" MajorTopicYN=\"N\">Behavior</DescriptorName>\n",
       "</MeshHeading>, <MeshHeading>\n",
       "<DescriptorName UI=\"D000013\" MajorTopicYN=\"N\">Congenital Abnormalities</DescriptorName>\n",
       "</MeshHeading>, <MeshHeading>\n",
       "<DescriptorName UI=\"D006233\" MajorTopicYN=\"N\">Disabled Persons</DescriptorName>\n",
       "</MeshHeading>, <MeshHeading>\n",
       "<DescriptorName UI=\"D004194\" MajorTopicYN=\"N\">Disease</DescriptorName>\n",
       "</MeshHeading>, <MeshHeading>\n",
       "<DescriptorName UI=\"D008607\" MajorTopicYN=\"Y\">Intellectual Disability</DescriptorName>\n",
       "</MeshHeading>, <MeshHeading>\n",
       "<DescriptorName UI=\"D007360\" MajorTopicYN=\"N\">Intelligence</DescriptorName>\n",
       "</MeshHeading>, <MeshHeading>\n",
       "<DescriptorName UI=\"D008431\" MajorTopicYN=\"Y\">Maternal-Fetal Exchange</DescriptorName>\n",
       "</MeshHe..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem \\\\ \"MeshHeading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: scala.collection.immutable.Seq[String] = List(Behavior, Congenital Abnormalities, Disabled Persons, Disease, Intellectual Disability, Intelligence, Maternal-Fetal Exchange, Personality, Pregnancy, Pregnancy Complications, Psychology, Reproduction, Research)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(elem \\\\ \"DescriptorName\").map(_.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "majorTopics: (record: String)Seq[String]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majorTopics(record:String):Seq[String] = {\n",
    "    val elem = XML.loadString(record)\n",
    "    val dn = elem \\\\\"DescriptorName\"\n",
    "    val mt = dn.filter(n => (n \\ \"@MajorTopicYN\").text==\"Y\")\n",
    "    mt.map(n => n.text)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medline: org.apache.spark.sql.Dataset[Seq[String]] = [value: array<string>]\n",
       "res5: Seq[String] = List(Intellectual Disability, Maternal-Fetal Exchange, Pregnancy Complications)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val medline = medlineRaw.map(majorTopics)\n",
    "medline.cache()\n",
    "medline.take(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics: org.apache.spark.sql.DataFrame = [topic: string]\n",
       "topicDist: org.apache.spark.sql.DataFrame = [topic: string, cnt: bigint]\n",
       "res6: Long = 7699\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medline.count()\n",
    "val topics = medline.flatMap(mesh => mesh).toDF(\"topic\")\n",
    "topics.createOrReplaceTempView(\"topics\")\n",
    "val topicDist = spark.sql(\"\"\"\n",
    "SELECT topic, COUNT(*) cnt\n",
    "FROM topics\n",
    "GROUP BY topic\n",
    "ORDER BY cnt DESC\"\"\")\n",
    "topicDist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               topic| cnt|\n",
      "+--------------------+----+\n",
      "|             Disease|1202|\n",
      "|           Neoplasms| 983|\n",
      "|        Tuberculosis| 950|\n",
      "|               Blood| 518|\n",
      "|          Anesthesia| 379|\n",
      "| Wounds and Injuries| 370|\n",
      "|            Medicine| 322|\n",
      "|Nervous System Ph...| 318|\n",
      "|Antibiotics, Anti...| 298|\n",
      "|         Body Fluids| 284|\n",
      "| Biomedical Research| 283|\n",
      "|               Brain| 276|\n",
      "|Congenital Abnorm...| 271|\n",
      "|Cardiovascular Sy...| 261|\n",
      "|         Vaccination| 251|\n",
      "|        Bacteriology| 247|\n",
      "|               Urine| 245|\n",
      "|Anti-Bacterial Ag...| 239|\n",
      "|              Niacin| 236|\n",
      "|             History| 235|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicDist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(1, 2)\n",
      "List(1, 3)\n",
      "List(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list: List[Int] = List(1, 2, 3)\n",
       "combs: Iterator[List[Int]] = empty iterator\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val list = List(1,2,3)\n",
    "val combs = list.combinations(2)\n",
    "combs.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(3, 2)\n",
      "List(3, 1)\n",
      "List(2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "combs: Iterator[List[Int]] = empty iterator\n",
       "res9: Boolean = false\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val combs = list.reverse.combinations(2)\n",
    "combs.foreach(println)\n",
    "List(3,2) == List(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicPairs: org.apache.spark.sql.DataFrame = [pairs: array<string>]\n",
       "cooccurs: org.apache.spark.sql.DataFrame = [pairs: array<string>, cnt: bigint]\n",
       "res10: Long = 72560\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicPairs = medline.flatMap(t => {\n",
    "    t.sorted.combinations(2)\n",
    "}).toDF(\"pairs\")\n",
    "topicPairs.createOrReplaceTempView(\"topic_pairs\")\n",
    "val cooccurs = spark.sql(\"\"\"\n",
    "SELECT pairs, COUNT(*) cnt\n",
    "FROM topic_pairs\n",
    "GROUP BY pairs\"\"\")\n",
    "cooccurs.cache()\n",
    "cooccurs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedArray(Antibiotics, Antitubercular, Dermatologic Agents),195]\n",
      "[WrappedArray(Analgesia, Anesthesia),181]\n",
      "[WrappedArray(Analgesia, Anesthesia and Analgesia),179]\n",
      "[WrappedArray(Anesthesia, Anesthesia and Analgesia),177]\n",
      "[WrappedArray(Anesthesia, Anesthesiology),153]\n",
      "[WrappedArray(Nervous System Physiological Phenomena, Reflex),151]\n",
      "[WrappedArray(Body Fluids, Urine),149]\n",
      "[WrappedArray(Antibodies, Antigens),133]\n",
      "[WrappedArray(Niacin, Tuberculosis),131]\n",
      "[WrappedArray(Isoniazid, Niacin),129]\n"
     ]
    }
   ],
   "source": [
    "cooccurs.createOrReplaceTempView(\"cooccurs\")\n",
    "spark.sql(\"\"\"SELECT pairs, cnt\n",
    "FROM cooccurs\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10\"\"\").collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.nio.charset.StandardCharsets\n",
       "import java.security.MessageDigest\n",
       "hashId: (str: String)Long\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.charset.StandardCharsets\n",
    "import java.security.MessageDigest\n",
    "\n",
    "def hashId(str: String): Long = {\n",
    "    val bytes = MessageDigest.getInstance(\"MD5\").\n",
    "    digest(str.getBytes(StandardCharsets.UTF_8))\n",
    "    (bytes(0) & 0xFFL) |\n",
    "    ((bytes(1) & 0xFFL) << 8) |\n",
    "    ((bytes(2) & 0xFFL) << 16) |\n",
    "    ((bytes(3) & 0xFFL) << 24) |\n",
    "    ((bytes(4) & 0xFFL) << 32) |\n",
    "    ((bytes(5) & 0xFFL) << 40) |\n",
    "    ((bytes(6) & 0xFFL) << 48) |\n",
    "    ((bytes(7) & 0xFFL) << 56) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "vertices: org.apache.spark.sql.DataFrame = [hash: bigint, topic: string]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val vertices = topics.map{ case Row(topic: String) =>\n",
    "    (hashId(topic),topic)}.toDF(\"hash\", \"topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n",
       "edges: org.apache.spark.sql.Dataset[org.apache.spark.graphx.Edge[Long]] = [srcId: bigint, dstId: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "val edges = cooccurs.map{case Row(topics: Seq[_], cnt: Long) =>\n",
    "val ids = topics.map(_.toString).map(hashId).sorted\n",
    "Edge(ids(0),ids(1),cnt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexRDD: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[84] at map at <console>:41\n",
       "topicGraph: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@6a9fb0c0\n",
       "res12: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@6a9fb0c0\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexRDD = vertices.rdd.map{\n",
    "    case Row(hash: Long, topic: String) => (hash, topic)\n",
    "}\n",
    "val topicGraph = Graph(vertexRDD, edges.rdd)\n",
    "topicGraph.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Long = 82195\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertexRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Long = 7699\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicGraph.vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "connectedComponentGraph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Long] = org.apache.spark.graphx.impl.GraphImpl@7757f20d\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val connectedComponentGraph = topicGraph.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 cid|count|\n",
      "+--------------------+-----+\n",
      "|-9215470674759766104| 7572|\n",
      "| 1765411469112156596|    3|\n",
      "| 3608770526546285755|    3|\n",
      "| 6617131068913763726|    2|\n",
      "| 2277126264016938007|    2|\n",
      "|-5293832349673873297|    2|\n",
      "|-7323847459013211825|    2|\n",
      "|-4492732731552733030|    2|\n",
      "|-7059326802402716265|    2|\n",
      "|  731949936574312042|    2|\n",
      "|-2551964588563249305|    2|\n",
      "| 5826763844776297056|    2|\n",
      "| 7056830101856982450|    1|\n",
      "| 3397189176630022717|    1|\n",
      "| 6103591236614385837|    1|\n",
      "| 2893116236831620270|    1|\n",
      "| 3697065331967883755|    1|\n",
      "| 3757970659535796638|    1|\n",
      "|  -52388043055878779|    1|\n",
      "| 1993830135517246392|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "componentDF: org.apache.spark.sql.DataFrame = [vid: bigint, cid: bigint]\n",
       "componentCounts: org.apache.spark.sql.DataFrame = [cid: bigint, count: bigint]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val componentDF = connectedComponentGraph.vertices.toDF(\"vid\",\"cid\")\n",
    "val componentCounts = componentDF.groupBy(\"cid\").count()\n",
    "componentCounts.orderBy($\"count\".desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicComponentDF: org.apache.spark.sql.DataFrame = [topic: string, cid: bigint]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicComponentDF = topicGraph.vertices.innerJoin(\n",
    "    connectedComponentGraph.vertices) {\n",
    "    (topicid, name, componentId) => (name, componentId.toLong)\n",
    "    }.values.toDF(\"topic\",\"cid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|      topic|                cid|\n",
      "+-----------+-------------------+\n",
      "|Herpestidae|1765411469112156596|\n",
      "|   Isospora|1765411469112156596|\n",
      "|   Coccidia|1765411469112156596|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicComponentDF.where(\"cid = 1765411469112156596\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|cnt|dist|\n",
      "+---+----+\n",
      "|  1|2347|\n",
      "|  2|1136|\n",
      "|  3| 724|\n",
      "|  4| 487|\n",
      "|  5| 379|\n",
      "|  6| 307|\n",
      "|  7| 199|\n",
      "|  8| 193|\n",
      "|  9| 169|\n",
      "| 10| 141|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicDist.createOrReplaceTempView(\"topic_dist\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT cnt, COUNT(*) dist\n",
    "FROM topic_dist\n",
    "GROUP BY cnt\n",
    "ORDER BY dist DESC\n",
    "LIMIT 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|      topic|cnt|\n",
      "+-----------+---+\n",
      "|Herpestidae|  1|\n",
      "+-----------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "campy: org.apache.spark.sql.DataFrame = [topic: string, cnt: bigint]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val campy = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM topic_dist\n",
    "WHERE topic LIKE '%Herpestidae%'\"\"\")\n",
    "campy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "degrees: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[262] at RDD at VertexRDD.scala:57\n",
       "res19: org.apache.spark.util.StatCounter = (count: 7596, mean: 19.104792, stdev: 40.490526, max: 1482.000000, min: 1.000000)\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val degrees: VertexRDD[Int] = topicGraph.degrees.cache()\n",
    "degrees.map(_._2).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namesAndDegrees: org.apache.spark.sql.DataFrame = [topic: string, degree: int]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val namesAndDegrees = degrees.innerJoin(topicGraph.vertices) {\n",
    "    (topicId, degree, name) => (name, degree.toInt)\n",
    "}.values.toDF(\"topic\",\"degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               topic|degree|\n",
      "+--------------------+------+\n",
      "|             Disease|  1482|\n",
      "|           Neoplasms|   918|\n",
      "|               Blood|   706|\n",
      "|        Tuberculosis|   665|\n",
      "| Wounds and Injuries|   449|\n",
      "|         Body Fluids|   396|\n",
      "|               Brain|   391|\n",
      "|Congenital Abnorm...|   371|\n",
      "|Antibiotics, Anti...|   370|\n",
      "|          Anesthesia|   356|\n",
      "|            Research|   354|\n",
      "|               Urine|   345|\n",
      "|            Syndrome|   344|\n",
      "|        Bacteriology|   339|\n",
      "|Diagnosis, Differ...|   333|\n",
      "|Physiological Pro...|   328|\n",
      "| Biomedical Research|   324|\n",
      "|Cardiovascular Sy...|   323|\n",
      "|              Plants|   309|\n",
      "|       Skin Diseases|   309|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "namesAndDegrees.orderBy($\"degree\".desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T: Long = 30000\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val T = medline.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicDistRdd: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[299] at rdd at <console>:43\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicDistRdd = topicDist.map{\n",
    "    case Row(topic: String, cnt: Long) => (hashId(topic),cnt)\n",
    "}.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicDistGraph: org.apache.spark.graphx.Graph[Long,Long] = org.apache.spark.graphx.impl.GraphImpl@b76287f\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicDistGraph = Graph(topicDistRdd, topicGraph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSq: (YY: Long, YB: Long, YA: Long, T: Long)Double\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chiSq(YY: Long, YB: Long, YA: Long, T: Long): Double = {\n",
    "    val NB = T-YB\n",
    "    val NA = T-YA\n",
    "    val YN = YA-YY\n",
    "    val NY = YB-YY\n",
    "    val NN = T-NY-YN-YY\n",
    "    val inner = math.abs(YY * NN - YN * NY) - T / 2.0\n",
    "    T * math.pow(inner,2)/(YA*NA*YB*NB)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSquaredGraph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@2b21f1f2\n",
       "res21: org.apache.spark.util.StatCounter = (count: 72560, mean: 315.119299, stdev: 1354.301605, max: 29096.810219, min: 0.000000)\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSquaredGraph = topicDistGraph.mapTriplets(triplet => {\n",
    "    chiSq(triplet.attr, triplet.srcAttr, triplet.dstAttr, T)})\n",
    "    chiSquaredGraph.edges.map(x => x.attr).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interesting: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@20a74ac7\n",
       "res22: Long = 36938\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interesting = chiSquaredGraph.subgraph(triplet => triplet.attr > 19.5)\n",
    "interesting.edges.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interestingComponentGraph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Double] = org.apache.spark.graphx.impl.GraphImpl@3bd75833\n",
       "icDF: org.apache.spark.sql.DataFrame = [vid: bigint, cid: bigint]\n",
       "icCountDF: org.apache.spark.sql.DataFrame = [cid: bigint, count: bigint]\n",
       "res23: Long = 125\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interestingComponentGraph = interesting.connectedComponents()\n",
    "val icDF = interestingComponentGraph.vertices.toDF(\"vid\",\"cid\")\n",
    "val icCountDF = icDF.groupBy(\"cid\").count()\n",
    "icCountDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 cid|count|\n",
      "+--------------------+-----+\n",
      "|-9215470674759766104| 7561|\n",
      "| 1765411469112156596|    3|\n",
      "| 3608770526546285755|    3|\n",
      "|-2551964588563249305|    2|\n",
      "|  731949936574312042|    2|\n",
      "|-7323847459013211825|    2|\n",
      "|-4492732731552733030|    2|\n",
      "|-5293832349673873297|    2|\n",
      "|-7285954876885413962|    2|\n",
      "| 6617131068913763726|    2|\n",
      "|-7059326802402716265|    2|\n",
      "| 2277126264016938007|    2|\n",
      "| 5826763844776297056|    2|\n",
      "| 2893116236831620270|    1|\n",
      "| 7303824060009723512|    1|\n",
      "| 7056830101856982450|    1|\n",
      "| 6103591236614385837|    1|\n",
      "|  -52388043055878779|    1|\n",
      "| 1993830135517246392|    1|\n",
      "|-4558125522602794424|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "icCountDF.orderBy($\"count\".desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interestingDegrees: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[484] at RDD at VertexRDD.scala:57\n",
       "res25: org.apache.spark.util.StatCounter = (count: 7587, mean: 9.737182, stdev: 10.634602, max: 247.000000, min: 1.000000)\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interestingDegrees = interesting.degrees.cache()\n",
    "interestingDegrees.map(_._2).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               topic|degree|\n",
      "+--------------------+------+\n",
      "|             Disease|   247|\n",
      "|           Neoplasms|   212|\n",
      "|              Plants|   134|\n",
      "|     Fractures, Bone|   103|\n",
      "|        Bacteriology|    94|\n",
      "|             Viruses|    88|\n",
      "|            Bacteria|    83|\n",
      "|Congenital Abnorm...|    82|\n",
      "|            Arteries|    79|\n",
      "|          Microscopy|    77|\n",
      "| Wounds and Injuries|    77|\n",
      "|Anesthesia and An...|    76|\n",
      "|               Brain|    69|\n",
      "|        Tuberculosis|    68|\n",
      "|               Cysts|    68|\n",
      "|          Antibodies|    67|\n",
      "|           Analgesia|    67|\n",
      "|            Antigens|    67|\n",
      "|          Anesthesia|    67|\n",
      "|Neoplasms, Experi...|    66|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interestingDegrees.innerJoin(topicGraph.vertices){\n",
    "    (topicId, degree, name) => (name, degree)\n",
    "}.values.toDF(\"topic\",\"degree\").orderBy($\"degree\".desc).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
