{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1602421834037)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.conf.Configuration\n",
       "import org.apache.hadoop.io.{LongWritable, Text}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import scala.collection.JavaConverters._\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import java.util.Properties\n",
       "import edu.umd.cloud9.collection.XMLInputFormat\n",
       "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
       "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io.{LongWritable, Text}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import java.util.Properties\n",
    "\n",
    "import edu.umd.cloud9.collection.XMLInputFormat\n",
    "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
    "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Data/Wikipedia-Geometry.xml\n",
       "conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "kvs: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = Data/Wikipedia-Geometry.xml NewHadoopRDD[0] at newAPIHadoopFile at <console>:43\n",
       "rawXmls: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"Data/Wikipedia-Geometry.xml\"\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XMLInputFormat.START_TAG_KEY, \"<page>\")\n",
    "conf.set(XMLInputFormat.END_TAG_KEY, \"</page>\")\n",
    "val kvs = spark.sparkContext.newAPIHadoopFile(path, classOf[XMLInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
    "val rawXmls = kvs.map(_._2.toString).toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.ByteArrayInputStream\n",
       "import info.bliki.wiki.dump._\n",
       "import org.xml.sax.SAXException\n",
       "import spark.implicits._\n",
       "defined class Page\n",
       "defined class ArticleFilter\n",
       "wikiXmlToPlainText: (pageXml: String)Option[(String, String)]\n",
       "docTexts: org.apache.spark.sql.Dataset[(String, String)] = [_1: string, _2: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.ByteArrayInputStream;\n",
    "import info.bliki.wiki.dump._\n",
    "import org.xml.sax.SAXException\n",
    "import spark.implicits._\n",
    "\n",
    "case class Page(var page: WikiArticle = new WikiArticle) {}\n",
    "\n",
    "class ArticleFilter(val Page: Page) extends IArticleFilter {\n",
    "    @throws(classOf[SAXException])\n",
    "    def process(page: WikiArticle, siteinfo: Siteinfo) {\n",
    "        Page.page = page\n",
    "    }\n",
    "}\n",
    "\n",
    "def wikiXmlToPlainText(pageXml: String): Option[(String, String)] = {\n",
    "    val Page = new Page\n",
    "    try{\n",
    "        val parser = new WikiXMLParser(new ByteArrayInputStream(pageXml.getBytes),new ArticleFilter(Page))\n",
    "        parser.parse()\n",
    "    }catch{\n",
    "        case e: Exception =>\n",
    "    }\n",
    "    val page = Page.page\n",
    "    if(page.getText != null && page.getTitle != null\n",
    "      && page.getId != null && page.getRevisionId != null\n",
    "      && page.getTimeStamp != null && !page.isTemplate) {\n",
    "        Some((page.getTitle, page.getText))\n",
    "    } else{\n",
    "        None\n",
    "    }\n",
    "}\n",
    "val docTexts = rawXmls.filter(_ != null).flatMap(wikiXmlToPlainText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createNLPPipeline: ()edu.stanford.nlp.pipeline.StanfordCoreNLP\n",
       "isOnlyLetters: (str: String)Boolean\n",
       "plainTextToLemmas: (text: String, stopWords: Set[String], pipeline: edu.stanford.nlp.pipeline.StanfordCoreNLP)Seq[String]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createNLPPipeline(): StanfordCoreNLP = {\n",
    "    val props = new Properties()\n",
    "    props.put(\"annotators\",\"tokenize, ssplit, pos, lemma\")\n",
    "    new StanfordCoreNLP(props)\n",
    "}\n",
    "\n",
    "def isOnlyLetters(str: String): Boolean = {\n",
    "    str.forall(c => Character.isLetter(c))\n",
    "}\n",
    "\n",
    "def plainTextToLemmas(text: String, stopWords: Set[String],\n",
    "    pipeline: StanfordCoreNLP): Seq[String] = {\n",
    "    val doc = new Annotation(text)\n",
    "    pipeline.annotate(doc)\n",
    "    \n",
    "    val lemmas = new ArrayBuffer[String]()\n",
    "    val sentences = doc.get(classOf[SentencesAnnotation])\n",
    "    for (sentence <- sentences.asScala;\n",
    "        token <- sentence.get(classOf[TokensAnnotation]).asScala){\n",
    "        val lemma = token.get(classOf[LemmaAnnotation])\n",
    "        if(lemma.length > 2 && !stopWords.contains(lemma)\n",
    "          && isOnlyLetters(lemma)){\n",
    "          lemmas += lemma.toLowerCase\n",
    "        }\n",
    "    }\n",
    "    lemmas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWords: scala.collection.immutable.Set[String] = Set(down, it's, ourselves, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, with, ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWords = scala.io.Source.fromFile(\"Data/stopwords.txt\").getLines().toSet\n",
    "val bStopWords = spark.sparkContext.broadcast(stopWords)\n",
    "\n",
    "val terms: Dataset[(String, Seq[String])] = \n",
    " docTexts.mapPartitions { iter =>\n",
    "     val pipeline = createNLPPipeline()\n",
    "     iter.map { case(title, contents) =>\n",
    "        (title, plainTextToLemmas(contents, bStopWords.value, pipeline))\n",
    "     }\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termsDF: org.apache.spark.sql.DataFrame = [title: string, terms: array<string>]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termsDF = terms.toDF(\"title\", \"terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [title: string, terms: array<string>]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered = termsDF.where(size($\"terms\") > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|               terms|\n",
      "+--------------------+--------------------+\n",
      "|Category:Discrete...|[portal, mathemat...|\n",
      "|Category:Incidenc...|[commons, categor...|\n",
      "|Category:Metric g...|[commons, categor...|\n",
      "|Category:Integral...|[category, geomet...|\n",
      "|Category:Conforma...|[cat, main, categ...|\n",
      "|Category:Trigonom...|[commons, cat, tr...|\n",
      "|Category:Convex g...|[commons, categor...|\n",
      "|Category:Technica...|[cat, main, techn...|\n",
      "|   Category:Symmetry|[commons, cat, sy...|\n",
      "|Category:Homogene...|[cat, main, homog...|\n",
      "|       Ambient space|[short, descripti...|\n",
      "|Category:Duality ...|[portal, mathemat...|\n",
      "|          Superspace|[superspace, coor...|\n",
      "|     Geometry Center|[geometry, center...|\n",
      "|          Dehn plane|[geometry, dehn, ...|\n",
      "|Complex reflectio...|[mathematics, com...|\n",
      "|Category:Geometri...|[commons, cat, ma...|\n",
      "|    Lipschitz domain|[mathematics, lip...|\n",
      "|        Complex line|[mathematics, com...|\n",
      "|Visibility (geome...|[visibility, geom...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numTerms: Int = 20000\n",
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_8baec16221d2\n",
       "vocabModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_8baec16221d2\n",
       "docTermFreqs: org.apache.spark.sql.DataFrame = [title: string, terms: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numTerms = 20000\n",
    "val countVectorizer = new CountVectorizer().\n",
    "    setInputCol(\"terms\").\n",
    "    setOutputCol(\"termFreqs\").\n",
    "    setVocabSize(numTerms)\n",
    "val vocabModel = countVectorizer.fit(filtered)\n",
    "val docTermFreqs = vocabModel.transform(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: docTermFreqs.type = [title: string, terms: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTermFreqs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_6aad8f089a2a\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = idf_6aad8f089a2a\n",
       "docTermMatrix: org.apache.spark.sql.DataFrame = [title: string, tfidfVec: vector]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF().\n",
    "    setInputCol(\"termFreqs\").\n",
    "    setOutputCol(\"tfidfVec\")\n",
    "val idfModel = idf.fit(docTermFreqs)\n",
    "val docTermMatrix = idfModel.transform(docTermFreqs).select(\"title\",\"tfidfVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termIds: Array[String] = Array(geometry, point, space, title, line, journal, cite, group, math, plane, page, two, date, can, first, use, volume, category, frac, url, right, one, mathcal, surface, year, angle, set, vector, file, image, theory, mathematics, last, shape, publisher, isbn, book, doi, also, metric, give, time, cdot, define, function, left, field, form, triangle, theorem, symmetry, dimensional, phi, coordinate, number, example, axis, case, computational, see, issue, minkowski, geodesic, method, equation, thumb, parallel, circle, operatorname, distance, map, structure, geometric, three, dimension, pattern, model, mathbf, call, anatomy, base, mathematical, object, mathbb, transformation, problem, follow, circ, varphi, author, area, length, manifold, end, unit, euclidean, system,..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termIds: Array[String] = vocabModel.vocabulary\n",
    "val docIds = docTermFreqs.rdd.map(_.getString(0)).\n",
    "    zipWithUniqueId().\n",
    "    map(_.swap).\n",
    "    collect().toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.{Vectors, Vector=>MLLibVector}\n",
       "import org.apache.spark.ml.linalg.{Vector=>MLVector}\n",
       "vecRdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[52] at map at <console>:52\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vectors, Vector => MLLibVector}\n",
    "import org.apache.spark.ml.linalg.{Vector => MLVector}\n",
    "\n",
    "val vecRdd = docTermMatrix.select(\"tfidfVec\").rdd.map { row =>\n",
    "Vectors.fromML(row.getAs[MLVector](\"tfidfVec\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
       "mat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@4b736e8a\n",
       "k: Int = 500\n",
       "svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] =\n",
       "SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@1b907a40,[1256.4621355939073,929.0766540782947,833.2957789686252,678.4654323957276,656.4446372483457,643.6494433768422,617.6619508502157,564.2799052331369,521.7230256071273,520.0396076459301,503.30050831911143,463.8714120045462,423.29347308036415,406.7361268552447,391.17122596442886,371.75197242292796,345.111715344905,329.8404563381224,324.66019551297796,309.24073158383..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "vecRdd.cache()\n",
    "val mat = new RowMatrix(vecRdd)\n",
    "val k = 500\n",
    "val svd = mat.computeSVD(k, computeU=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.{Matrix, SingularValueDecomposition}\n",
       "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
       "topTermsInTopConcepts: (svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix], numConcepts: Int, numTerms: Int, termIds: Array[String])Seq[Seq[(String, Double)]]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Matrix,SingularValueDecomposition}\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "def topTermsInTopConcepts(\n",
    "svd: SingularValueDecomposition[RowMatrix, Matrix],\n",
    "numConcepts: Int,\n",
    "numTerms: Int, termIds: Array[String])\n",
    ": Seq[Seq[(String, Double)]] = {\n",
    "val v = svd.V\n",
    "val topTerms = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "val arr = v.toArray\n",
    "for (i <- 0 until numConcepts) {\n",
    "    val offs = i * v.numRows\n",
    "    val termWeights = arr.slice(offs, offs + v.numRows).zipWithIndex\n",
    "    val sorted = termWeights.sortBy(-_._1)\n",
    "    topTerms += sorted.take(numTerms).map {\n",
    "        case (score, id) => (termIds(id), score)\n",
    "    }\n",
    "}\n",
    "topTerms\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topDocsInTopConcepts: (svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix], numConcepts: Int, numDocs: Int, docIds: Map[Long,String])Seq[Seq[(String, Double)]]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topDocsInTopConcepts(\n",
    "svd: SingularValueDecomposition[RowMatrix, Matrix],\n",
    "numConcepts: Int, numDocs: Int, docIds:Map[Long, String])\n",
    ": Seq[Seq[(String, Double)]]= {\n",
    "val u = svd.U\n",
    "val topDocs = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "for (i <- 0 until numConcepts) {\n",
    "    val docWeights = u.rows.map(_.toArray(i)).zipWithUniqueId()\n",
    "    topDocs += docWeights.top(numDocs).map{\n",
    "        case (score, id) => (docIds(id), score)\n",
    "    }\n",
    "}\n",
    "topDocs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept terms: category, geometry, catmain, newstub, wpss, stereochemistry\n",
      "Concept docs: Category:Inversive geometry, Category:Conformal geometry, Category:Analytic geometry, Category:Geometry stubs, Category:Theorems in geometry, Category:Molecular geometry\n",
      "\n",
      "Concept terms: snub, cdd, bmatrix, node, uniform, tiling\n",
      "Concept docs: Snub (geometry), Complex reflection group, Minkowski space, Spacetime diagram, Busemann function, Moir? pattern\n",
      "\n",
      "Concept terms: minkowski, math, mathbf, spacetime, vector, relativity\n",
      "Concept docs: Minkowski space, Spacetime diagram, Busemann function, Moir? pattern, Geometry, Fat object (geometry)\n",
      "\n",
      "Concept terms: minkowski, mathbf, vector, math, spacetime, eta\n",
      "Concept docs: Minkowski space, Computational anatomy, Valuation (geometry), Spacetime diagram, Riemannian metric and Lie bracket in computational anatomy, Complex reflection group\n",
      "\n",
      "Concept terms: moir, layer, pattern, minkowski, mathbf, spacetime\n",
      "Concept docs: Moir? pattern, Line moir?, Minkowski space, Perspective geological correlation, Spacetime diagram, Computational anatomy\n",
      "\n",
      "Concept terms: val, valuation, operatorname, mathcal, moir, phi\n",
      "Concept docs: Valuation (geometry), Moir? pattern, Line moir?, Perspective geological correlation, Base change theorems, Laguerre plane\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topConceptTerms: Seq[Seq[(String, Double)]] = ArrayBuffer(ArraySeq((category,3.2526065174565133E-19), (geometry,-2.1819568721270777E-18), (catmain,-5.506282006739749E-8), (newstub,-5.616726082803211E-8), (wpss,-5.616726085166772E-8), (stereochemistry,-9.011149715819344E-8)), ArraySeq((snub,0.5016816106506126), (cdd,0.4914580527147358), (bmatrix,0.4389003606247568), (node,0.26871997759467914), (uniform,0.15838182790459227), (tiling,0.15295434166125405)), ArraySeq((minkowski,0.3162818903503688), (math,0.3013590597490821), (mathbf,0.2397618662268306), (spacetime,0.2080337169047539), (vector,0.17937256351152658), (relativity,0.17062239618071576)), ArraySeq((minkowski,0.13621843201552586), (mathbf,0.11676383518907249), (vector,0.10539298227158342), (math,0.08723626570553503), (spacetime,0.08..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topConceptTerms = topTermsInTopConcepts(svd, 6, 6, termIds)\n",
    "val topConceptDocs = topDocsInTopConcepts(svd, 6, 6, docIds)\n",
    "for ((terms, docs) <- topConceptTerms.zip(topConceptDocs)){\n",
    "    println(\"Concept terms: \" + terms.map(_._1).mkString(\", \"))\n",
    "    println(\"Concept docs: \" + docs.map(_._1).mkString(\", \"))\n",
    "    println()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
